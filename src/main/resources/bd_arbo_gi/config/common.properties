###########################  bdl.  ##########################################
# cles obligatoires imposees par le cadre
# doivent commencer par "bdl."
#############################################################################

# liste des cles contenant des valeurs a ne pas afficher par les fonctions de logging
bdl.redaction=

# niveau de log pour les fonctions de logging (debug, info, warn ou error)
bdl.logger.level=info


###########################  app.  ##########################################
# Cles libres contenant les paramètres applicatifs/fonctionnels
# Spécifique à chaque projet.
# Le nombre de clés et leurs valeurs sont totalement libres.
# Elles doivent commencer par "app."
#############################################################################

# Les jobs dans YARN
app.yarn.queue=${GLB_POOL_RESSOURCE}
app.yarn.name.prefixe=${CODE_APPLI}${SUJET}_${INSTANCE}_${GLB_MODULE}
app.yarn.name={{app.yarn.name.prefixe}}_[suffixe_a_remplacer]

# fichier a utiliser pour log4j
app.log4j.file=log4j_mixte.properties

#********************#
#     COMPACTAGE     #
#********************#
cpt.queue_name={{app.yarn.queue}}
cpt.principal={{spk.common.principal}}
cpt.keytab={{spk.common.keytab}}

####################################################################################################
# Retention HDFS
####################################################################################################
app.purge.hdfs.retention=15

# DATABASE et PATH HDFS du RAW du SUJET du PROJET
app.database.raw=${DATABASE_RAW}
app.hdfs.path.raw.to_validate=${HDFS_RAW}/${GLB_MODULE}/to_validate
app.hdfs.path.raw.rejected=${HDFS_RAW}/${GLB_MODULE}/rejected
app.hdfs.path.raw.to_process=${HDFS_RAW}/${GLB_MODULE}/to_process
app.hdfs.path.raw.failed=${HDFS_RAW}/${GLB_MODULE}/failed
app.hdfs.path.raw.compacted=${HDFS_RAW}/${GLB_MODULE}/compacted
app.hdfs.path.raw.processed=${HDFS_RAW}/${GLB_MODULE}/processed

# DATABASE et PATH HDFS du SAFE du SUJET du PROJET
app.database.safe=${DATABASE_SAFE}
app.hdfs.path.safe=${HDFS_SAFE}/${GLB_MODULE}

# DATABASE et PATH HDFS de l'OPTIMIZED du SUJET  du PROJET
app.database.optimized=${DATABASE_DISTRI_OPTIMIZED}
app.hdfs.path.optimized=${HDFS_DISTRI_OPTIMIZED}

# Table RAW optimiss Routing Request
app.table.raw.routingrequest=r_${GLB_MODULE}_opm_routingrequest_in_v1
app.{{app.table.raw.routingrequest}}.hdfs.path.processed={{app.hdfs.path.raw.processed}}/{{app.table.raw.routingrequest}}

# Table RAW optimiss Routing Result
app.table.raw.routingresult=r_${GLB_MODULE}_opm_routingresult_out_v1
app.{{app.table.raw.routingresult}}.hdfs.path.processed={{app.hdfs.path.raw.processed}}/{{app.table.raw.routingresult}}

# Table SAFE bronze optimiss tournee demande deltas
app.table.safe.bronze.tournee_demande_deltas=b_${GLB_MODULE}_tournee_demande_deltas
app.{{app.table.safe.bronze.tournee_demande_deltas}}.database={{app.database.safe}}
app.{{app.table.safe.bronze.tournee_demande_deltas}}.hdfs.path={{app.hdfs.path.safe}}/{{app.table.safe.bronze.tournee_demande_deltas}}
app.{{app.table.safe.bronze.tournee_demande_deltas}}.format=parquet
app.{{app.table.safe.bronze.tournee_demande_deltas}}.hql.create=${INSTALL}/hql/create_safe_tournee_demande_deltas.hql
app.{{app.table.safe.bronze.tournee_demande_deltas}}.spark.class=alimtournee_demande_deltas
cpt.{{app.table.safe.bronze.tournee_demande_deltas}}.queue_name=${GLB_POOL_RESSOURCES}
cpt.{{app.table.safe.bronze.tournee_demande_deltas}}.job_name={{app.yarn.name.prefixe}}_{{app.table.safe.bronze.tournee_demande_deltas}}_compact
cpt.{{app.table.safe.bronze.tournee_demande_deltas}}.mode_compression=snappy
cpt.{{app.table.safe.bronze.tournee_demande_deltas}}.taille_bloc=125
cpt.{{app.table.safe.bronze.tournee_demande_deltas}}.nb_fichiers=1
cpt.{{app.table.safe.bronze.tournee_demande_deltas}}.sparkparam.executor-cores=2
cpt.{{app.table.safe.bronze.tournee_demande_deltas}}.sparkparam.executor-memory=4G

# Table SAFE bronze optimiss tournee resultat deltas
app.table.safe.bronze.tournee_resultat_deltas=b_${GLB_MODULE}_tournee_resultat_deltas
app.{{app.table.safe.bronze.tournee_resultat_deltas}}.database={{app.database.safe}}
app.{{app.table.safe.bronze.tournee_resultat_deltas}}.hdfs.path={{app.hdfs.path.safe}}/{{app.table.safe.bronze.tournee_resultat_deltas}}
app.{{app.table.safe.bronze.tournee_resultat_deltas}}.format=parquet
app.{{app.table.safe.bronze.tournee_resultat_deltas}}.hql.create=${INSTALL}/hql/create_safe_tournee_resultat_deltas.hql
app.{{app.table.safe.bronze.tournee_resultat_deltas}}.spark.class=alimtournee_resultat_deltas
cpt.{{app.table.safe.bronze.tournee_resultat_deltas}}.queue_name=${GLB_POOL_RESSOURCES}
cpt.{{app.table.safe.bronze.tournee_resultat_deltas}}.job_name={{app.yarn.name.prefixe}}_{{app.table.safe.bronze.tournee_resultat_deltas}}_compact
cpt.{{app.table.safe.bronze.tournee_resultat_deltas}}.mode_compression=snappy
cpt.{{app.table.safe.bronze.tournee_resultat_deltas}}.taille_bloc=125
cpt.{{app.table.safe.bronze.tournee_resultat_deltas}}.nb_fichiers=1
cpt.{{app.table.safe.bronze.tournee_resultat_deltas}}.sparkparam.executor-cores=2
cpt.{{app.table.safe.bronze.tournee_resultat_deltas}}.sparkparam.executor-memory=4G




# Table SAFE bronze optimiss tournee demande
app.table.safe.bronze.tournee_demande=b_${GLB_MODULE}_tournee_demande
app.{{app.table.safe.bronze.tournee_demande}}.database={{app.database.safe}}
app.{{app.table.safe.bronze.tournee_demande}}.hdfs.path={{app.hdfs.path.safe}}/{{app.table.safe.bronze.tournee_demande}}
app.{{app.table.safe.bronze.tournee_demande}}.format=parquet
app.{{app.table.safe.bronze.tournee_demande}}.hql.create=${INSTALL}/hql/create_safe_tournee_demande.hql
cpt.{{app.table.safe.bronze.tournee_demande}}.queue_name=${GLB_POOL_RESSOURCES}
cpt.{{app.table.safe.bronze.tournee_demande}}.job_name={{app.yarn.name.prefixe}}_{{app.table.safe.bronze.tournee_demande}}_compact
cpt.{{app.table.safe.bronze.tournee_demande}}.mode_compression=snappy
cpt.{{app.table.safe.bronze.tournee_demande}}.taille_bloc=125
cpt.{{app.table.safe.bronze.tournee_demande}}.nb_fichiers=1
cpt.{{app.table.safe.bronze.tournee_demande}}.sparkparam.executor-cores=2
cpt.{{app.table.safe.bronze.tournee_demande}}.sparkparam.executor-memory=4G

# Table SAFE bronze optimiss tournee resultat
app.table.safe.bronze.tournee_resultat=b_${GLB_MODULE}_tournee_resultat
app.{{app.table.safe.bronze.tournee_resultat}}.database={{app.database.safe}}
app.{{app.table.safe.bronze.tournee_resultat}}.hdfs.path={{app.hdfs.path.safe}}/{{app.table.safe.bronze.tournee_resultat}}
app.{{app.table.safe.bronze.tournee_resultat}}.format=parquet
app.{{app.table.safe.bronze.tournee_resultat}}.hql.create=${INSTALL}/hql/create_safe_tournee_resultat.hql
cpt.{{app.table.safe.bronze.tournee_resultat}}.queue_name=${GLB_POOL_RESSOURCES}
cpt.{{app.table.safe.bronze.tournee_resultat}}.job_name={{app.yarn.name.prefixe}}_{{app.table.safe.bronze.tournee_resultat}}_compact
cpt.{{app.table.safe.bronze.tournee_resultat}}.mode_compression=snappy
cpt.{{app.table.safe.bronze.tournee_resultat}}.taille_bloc=125
cpt.{{app.table.safe.bronze.tournee_resultat}}.nb_fichiers=1
cpt.{{app.table.safe.bronze.tournee_resultat}}.sparkparam.executor-cores=2
cpt.{{app.table.safe.bronze.tournee_resultat}}.sparkparam.executor-memory=4G

# Table OPTIMIZED silver distri tournee optimiss
app.table.optimized.silver.tournee_optimiss=s_distri_tournee_${GLB_MODULE}
app.{{app.table.optimized.silver.tournee_optimiss}}.database={{app.database.optimized}}
app.{{app.table.optimized.silver.tournee_optimiss}}.hdfs.path={{app.hdfs.path.optimized}}/{{app.table.optimized.silver.tournee_optimiss}}
app.{{app.table.optimized.silver.tournee_optimiss}}.format=parquet
app.{{app.table.optimized.silver.tournee_optimiss}}.hql.create=${INSTALL}/hql/create_optimized_silver_distri_tournee_opimiss.hql

###########################  HIV.  ################################################################################
# PARAMÈTRES UTILISÉS PAR LA FONCTION HIVE_SUBMIT OU HIVE_ALIM POUR EXÉCUTION DU HQL DES TABLES SAFE              #
###################################################################################################################

# Alimentation TABLE SAFE tournee demande
hiv.{{app.table.safe.bronze.tournee_demande}}.hql=${BATCH}/hql/alim_safe_bronze_tournee_demande.hql
hiv.{{app.table.safe.bronze.tournee_demande}}.hiveconf.yarn.name={{app.yarn.name.prefixe}}_{{app.table.safe.bronze.tournee_demande}}_alim
hiv.{{app.table.safe.bronze.tournee_demande}}.hivevar.database_safe={{app.database.safe}}
hiv.{{app.table.safe.bronze.tournee_demande}}.hivevar.table_safe_tournee_demande={{app.table.safe.bronze.tournee_demande}}
hiv.{{app.table.safe.bronze.tournee_demande}}.hivevar.table_safe_tournee_demande_deltas={{app.table.safe.bronze.tournee_demande_deltas}}

# Alimentation TABLE SAFE tournee resultat
hiv.{{app.table.safe.bronze.tournee_resultat}}.hql=${BATCH}/hql/alim_safe_bronze_tournee_resultat.hql
hiv.{{app.table.safe.bronze.tournee_resultat}}.hiveconf.yarn.name={{app.yarn.name.prefixe}}_{{app.table.safe.bronze.tournee_resultat}}_alim
hiv.{{app.table.safe.bronze.tournee_resultat}}.hivevar.database_safe={{app.database.safe}}
hiv.{{app.table.safe.bronze.tournee_resultat}}.hivevar.table_safe_tournee_resultat={{app.table.safe.bronze.tournee_resultat}}
hiv.{{app.table.safe.bronze.tournee_resultat}}.hivevar.table_safe_tournee_resultat_deltas={{app.table.safe.bronze.tournee_resultat_deltas}}

# Alimentation Table OPTIMIZED silver distri tournee optimiss
hiv.{{app.table.optimized.silver.tournee_optimiss}}.hql=${BATCH}/hql/alim_optimized_silver_distri_tournee_opimiss.hql
hiv.{{app.table.optimized.silver.tournee_optimiss}}.hiveconf.yarn.name={{app.yarn.name.prefixe}}_{{app.table.optimized.silver.tournee_optimiss}}_alim
hiv.{{app.table.optimized.silver.tournee_optimiss}}.hivevar.database_optimized={{app.database.optimized}}
hiv.{{app.table.optimized.silver.tournee_optimiss}}.hivevar.database_safe={{app.database.safe}}
hiv.{{app.table.optimized.silver.tournee_optimiss}}.hivevar.table_optimized_tournee_optimiss={{app.table.optimized.silver.tournee_optimiss}}
hiv.{{app.table.optimized.silver.tournee_optimiss}}.hivevar.table_safe_tournee_demande={{app.table.safe.bronze.tournee_demande}}
hiv.{{app.table.optimized.silver.tournee_optimiss}}.hivevar.table_safe_tournee_resultat={{app.table.safe.bronze.tournee_resultat}}

#################  spk.  #####################################
# cles obligatoires pour piloter le lancement du binaire spark
# doivent commencer par "spk."
##############################################################

# binaire a lancer
spk.common.bin=${BDL_SPARK_CMD}

# nom qui doit apparaitre dans YARN
spk.common.name={{app.yarn.name}}

# nom du pool de ressources a utiliser
spk.common.queue={{app.yarn.queue}}

# type de deploiement
spk.common.master=yarn
spk.common.deploy-mode=cluster

# informations pour kerberos
spk.common.keytab=${KRB_USER_KEYTAB}
spk.common.principal=${KRB_USER_PRINCIPAL}

# parametrage du driver et des executors
spk.common.driver-cores=2
spk.common.driver-memory=4g
spk.common.conf.spark.driver.memoryOverhead=2048
spk.common.executor-cores=2
spk.common.executor-memory=6g
spk.common.conf.spark.executor.memoryOverhead=2048


# autre conf (libre, se referer a la doc Spark)
spk.common.conf.spark.logConf=true
spk.common.conf.spark.dynamicAllocation.enabled=true
spk.common.conf.spark.dynamicAllocation.initialExecutors=1
spk.common.conf.spark.dynamicAllocation.minExecutors=1
spk.common.conf.spark.dynamicAllocation.maxExecutors=2
spk.common.conf.spark.hadoop.fs.hdfs.impl.disable.cache=true
# https://spark.apache.org/docs/2.2.0/sql-programming-guide.html#upgrading-from-spark-sql-21-to-22
spk.common.conf.spark.sql.hive.caseSensitiveInferenceMode=NEVER_INFER

# fichiers a envoyer dans HDFS (au moins le fichier de conf log4j)
spk.common.files.01=${PROPERTIES}/{{app.log4j.file}}

# complement au classpath (au moins ceux ci-dessous)
spk.common.driver_executor.extraClassPath.01=.
spk.common.driver_executor.extraClassPath.02=${PROPERTIES}

# options java a transmettre au driver (au moins celles ci-dessous)
spk.common.driver.extraJavaOptions.log4j.configuration={{app.log4j.file}}
spk.common.driver.extraJavaOptions.sun.security.krb5.debug=false

# options java a transmettre aux executors (au moins celles ci-dessous)
spk.common.executor.extraJavaOptions.log4j.configuration={{app.log4j.file}}
spk.common.executor.extraJavaOptions.sun.security.krb5.debug=false


# parametrage HIVE dans un contexte SPARK
spk.common.conf.spark.hadoop.hive.metastore.connect.retries=3600
spk.common.conf.spark.hadoop.hive.metastore.client.connect.retry.delay=1s
spk.common.conf.spark.hadoop.hive.exec.dynamic.partition=true
spk.common.conf.spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict
spk.common.conf.spark.hadoop.hive.exec.max.dynamic.partitions=2000
spk.common.conf.spark.hadoop.hive.exec.max.dynamic.partitions.pernode=2000

# classe main
spk.spark.alimtournee_demande_deltas.class=fr.laposte.bdl.bscc.${GLB_MODULE}.spark.AlimSafeTourneeDemandeDeltas
spk.spark.alimtournee_demande_deltas.name={{app.yarn.name.prefixe}}_alim_safe_bronze_tournee_demande_deltas
spk.spark.alimtournee_resultat_deltas.class=fr.laposte.bdl.bscc.${GLB_MODULE}.spark.AlimSafeTourneeResultatDeltas
spk.spark.alimtournee_resultat_deltas.name={{app.yarn.name.prefixe}}_alim_safe_bronze_tournee_resultat_deltas

spk.common.conf.spark.yarn.maxAppAttempts=1


#################  suez  #####################################
# cles pour parametrer suezlib
# https://wiki.net.extra.laposte.fr/confluence/display/BGDB/SuezLib
##############################################################
app.suez.request.topics=opm_routingRequest_in_v1
app.suez.request.environnement=acc
app.suez.request.datacenter=mar
app.suez.request.lanceur=streaming
app.suez.request.offset.strategy=normal
app.suez.request.conversion=rawOnly
app.suez.request.table.processed.name.opm_routingRequest_in_v1={{app.table.raw.routingrequest}}
app.suez.request.table.processed.partition=true
app.suez.request.table.processed.partition.format=%Y-%m-%d

app.suez.result.topics=opm_routingResult_out_v1
app.suez.result.environnement=acc
app.suez.result.datacenter=mar
app.suez.result.lanceur=streaming
app.suez.result.offset.strategy=normal
app.suez.result.conversion=rawOnly
app.suez.result.table.processed.name.opm_routingResult_out_v1={{app.table.raw.routingresult}}
app.suez.result.table.processed.partition=true
app.suez.result.table.processed.partition.format=%Y-%m-%d

#parametres d'optimisation des streamings
app.suez.spk.common.executor-cores=3
app.suez.spk.common.executor-memory=6g
app.suez.spk.common.driver-memory=1g
app.suez.spk.common.conf.spark.streaming.kafka.maxRatePerPartition=290
app.suez.spk.common.conf.spark.memory.offHeap.size=2g
app.suez.spk.common.conf.spark.memory.offHeap.enabled=true

app.suez.suez.consumer.duration.sec=60




